{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example notebook for using the Multitask Neural Decoding code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.utilities import disable_possible_user_warnings\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M1_EMG_Dataset_Toy(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, num_samples, num_neurons, num_muscles, num_modes, batch_size, dataset_type):\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_muscles = num_muscles\n",
    "        self.num_modes = num_modes\n",
    "        self.batch_size = batch_size\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.dataset_type = dataset_type\n",
    "        self.decoder_mode1 = torch.randn(self.num_neurons, self.num_muscles)\n",
    "        self.decoder_mode2 = torch.randn(self.num_neurons, self.num_muscles)\n",
    "\n",
    "        # Generate toy features and labels\n",
    "        self.train_dataset = self.generate_toy_dataset(self.num_samples)\n",
    "        self.val_dataset = self.generate_toy_dataset(self.num_samples*0.2)\n",
    "\n",
    "\n",
    "    def generate_behavioral(self, num_samples, m1_val, mode):\n",
    "        \"\"\"\n",
    "        Helper function to generate data with behavioral labels for testing ClusterModel sub-model\n",
    "        \n",
    "        Input: (int) num_samples: number of samples\n",
    "            (float) m1_val: value of the features in this mode\n",
    "            (str) mode: which mode to generate\n",
    "        Output: ([num_samples, num_neurons] tensor) features: output features\n",
    "                ([num_samples] tensor) labels: output behavioral labels \n",
    "        \"\"\"\n",
    "        if mode == \"mode1\":\n",
    "            labels = F.one_hot(torch.zeros(num_samples, dtype=int), self.num_modes)\n",
    "        elif mode == \"mode2\":\n",
    "            labels = F.one_hot(torch.ones(num_samples, dtype=int), self.num_modes)\n",
    "\n",
    "        features = torch.full((num_samples, self.num_neurons), m1_val) + torch.randn(num_samples, self.num_neurons)\n",
    "            \n",
    "        return features, labels\n",
    "    \n",
    "    \n",
    "    def generate_emg(self, num_samples, m1_val, decoder):\n",
    "        \"\"\"\n",
    "        Helper function to generate data with EMG labels for testing the full CombinedModel\n",
    "        \n",
    "        Input: (int) num_samples: number of samples\n",
    "            (float) m1_val: value of the features in this mode\n",
    "        Output: ([num_samples, num_neurons] tensor) features: output features\n",
    "                ([num_samples, num_muscles] tensor) labels: output EMG labels \n",
    "        \"\"\"\n",
    "\n",
    "        features = torch.full((num_samples, self.num_neurons), m1_val) + torch.randn(num_samples, self.num_neurons)\n",
    "        labels = torch.matmul(features, decoder)\n",
    "\n",
    "        return features, labels\n",
    "\n",
    "\n",
    "    def generate_toy_dataset(self, num_samples):\n",
    "        \"\"\"\n",
    "        Generates the final toy dataset\n",
    "        Input: (int) num_samples: number of samples\n",
    "        Output: ((feature, label) tuple) dataset: output dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        # Dataset parameters\n",
    "        num_samples_total = num_samples\n",
    "        num_samples = int(num_samples_total/2)\n",
    "        m1_val_mode1 = 10.0\n",
    "        m1_val_mode2 = 0.0\n",
    "\n",
    "        # Generate toy dataset with behavioral labels to test out ClusterModel\n",
    "        if self.dataset_type == \"behavioral\":\n",
    "            features_mode1, labels_mode1 = self.generate_behavioral(num_samples, m1_val_mode1, \"mode1\")\n",
    "            features_mode2, labels_mode2 = self.generate_behavioral(num_samples, m1_val_mode2, \"mode2\")\n",
    "        \n",
    "        # Generate toy dataset with EMG labels to test out full CombinedModel\n",
    "        elif self.dataset_type == \"emg\":\n",
    "            features_mode1, labels_mode1 = self.generate_emg(num_samples, m1_val_mode1, self.decoder_mode1)\n",
    "            features_mode2, labels_mode2 = self.generate_emg(num_samples, m1_val_mode2, self.decoder_mode2)\n",
    "        \n",
    "        # Format datasets in pairs of (feature, label)\n",
    "        dataset_mode1 = [(features_mode1[i], labels_mode1[i]) for i in range(len(features_mode1))]\n",
    "        dataset_mode2 = [(features_mode2[i], labels_mode2[i]) for i in range(len(features_mode2))]\n",
    "        dataset = dataset_mode1 + dataset_mode2\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.train_dataset[index]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        final_batch = {}\n",
    "        X = []\n",
    "        Y = []\n",
    "        for sample in batch:\n",
    "            X.append(sample[0])\n",
    "            Y.append(sample[1])\n",
    "        final_batch[\"m1\"] = torch.stack(X)\n",
    "        final_batch[\"emg\"] = torch.stack(Y).float()\n",
    "        return final_batch\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering model\n",
    "class ClusterModel(nn.Module):\n",
    "    \"\"\"\n",
    "    input_dim: N\n",
    "    num_modes: d\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_modes):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(input_dim, 1) for i in range(num_modes)])\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_d = []\n",
    "        for linear in self.linears:\n",
    "            x_d.append(linear(x))\n",
    "        x = torch.stack(x_d, 2)\n",
    "        x = self.softmax(x) \n",
    "        return x\n",
    "    \n",
    "    \n",
    "# Decoding model\n",
    "class DecoderModel(nn.Module):\n",
    "    \"\"\"\n",
    "    input_dim: N\n",
    "    output_dim: M\n",
    "    num_modes: d\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, num_modes):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(input_dim, output_dim) for i in range(num_modes)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_d = []\n",
    "        for linear in self.linears:\n",
    "            x_d.append(linear(x))\n",
    "        x = torch.stack(x_d, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    \"\"\"\n",
    "    self.cm stores an instance of the cluster model\n",
    "    self.dm stores an instancee of the decoding model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_modes, ev):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.cm = ClusterModel(input_dim, num_modes)\n",
    "        self.dm = DecoderModel(input_dim, output_dim, num_modes)\n",
    "        self.ev = ev\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.cm(x)\n",
    "        x2 = self.dm(x)\n",
    "        output = torch.sum(x1 * x2, dim=-1)\n",
    "\n",
    "        # Return softmax outputs if mode is \"eval\"\n",
    "        if self.ev == True:\n",
    "            return x1\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingModule(LightningModule):\n",
    "\n",
    "    def __init__(self, model, lr, type):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.dataset_type = type\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        features = batch[\"m1\"]\n",
    "        labels = batch[\"emg\"]\n",
    "        labels_hat = self.model(features).squeeze()\n",
    "        train_loss = F.mse_loss(labels_hat, labels)\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        features = batch[\"m1\"]\n",
    "        labels = batch[\"emg\"]\n",
    "        labels_hat = self.model(features).squeeze()\n",
    "        val_loss = F.mse_loss(labels_hat, labels)\n",
    "        return val_loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/andrewshen/Desktop/neural_decoding/decoding_venv/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name  | Type          | Params\n",
      "----------------------------------------\n",
      "0 | model | CombinedModel | 132   \n",
      "----------------------------------------\n",
      "132       Trainable params\n",
      "0         Non-trainable params\n",
      "132       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n"
     ]
    }
   ],
   "source": [
    "# Load in dataset\n",
    "T = 1000\n",
    "N = 10\n",
    "M = 3\n",
    "d = 3  # num_modes\n",
    "b = 8\n",
    "type = \"emg\"\n",
    "epochs = 500\n",
    "lr = 0.0001\n",
    "save_path = \"checkpoints\"\n",
    "dataset = M1_EMG_Dataset_Toy(num_samples=T,\n",
    "                             num_neurons=N,\n",
    "                             num_muscles=M,\n",
    "                             num_modes=d,\n",
    "                             batch_size=b,\n",
    "                             dataset_type=type)\n",
    "\n",
    "# Define model\n",
    "model = CombinedModel(input_dim=N,\n",
    "                        output_dim=M,\n",
    "                        num_modes=d, \n",
    "                        ev=False)\n",
    "model = TrainingModule(model=model,\n",
    "                        lr=lr,\n",
    "                        type=type)\n",
    "\n",
    "# Define model checkpoints\n",
    "filename = \"checkpoints/checkpoints.ckpt\"\n",
    "if os.path.exists(filename):\n",
    "    os.remove(filename)\n",
    "save_callback = ModelCheckpoint(dirpath = save_path, filename=\"checkpoints\")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(max_epochs=epochs, callbacks=save_callback, enable_progress_bar=False)\n",
    "\n",
    "# Fit the model\n",
    "disable_possible_user_warnings()\n",
    "trainer.fit(model, train_dataloaders=dataset.train_dataloader(), val_dataloaders=dataset.val_dataloader())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned cluster probabilities:\n",
      "tensor([[    1.0000,     0.0000,     0.0000],\n",
      "        [    1.0000,     0.0000,     0.0000],\n",
      "        [    1.0000,     0.0000,     0.0000],\n",
      "        ...,\n",
      "        [    0.0153,     0.4129,     0.5718],\n",
      "        [    0.0138,     0.1447,     0.8415],\n",
      "        [    0.0333,     0.4064,     0.5604]])\n",
      "\n",
      "Final predictions:\n",
      "tensor([[    -0.9288,    -27.6711,      6.1406],\n",
      "        [     0.7430,    -28.4463,      5.9731],\n",
      "        [    -0.0005,    -26.6970,      6.7494],\n",
      "        ...,\n",
      "        [     1.0318,      0.6768,     -0.7112],\n",
      "        [     1.0472,      7.9783,      0.7533],\n",
      "        [     0.7715,      3.1245,      1.0610]])\n",
      "\n",
      "EMG values:\n",
      "tensor([[ -5.4241, -25.4261,   7.1858],\n",
      "        [ -0.6420, -29.8326,   5.9900],\n",
      "        [ -1.9857, -24.4516,   7.1309],\n",
      "        ...,\n",
      "        [  1.0294,   0.6697,  -0.7132],\n",
      "        [  1.0416,   7.9853,   0.7433],\n",
      "        [  0.7316,   3.1858,   1.0761]])\n"
     ]
    }
   ],
   "source": [
    "# Access train dataset\n",
    "train = dataset.train_dataset\n",
    "\n",
    "# Load trained model\n",
    "model_path = filename\n",
    "model = CombinedModel(input_dim=N,\n",
    "                      output_dim=M,\n",
    "                      num_modes=d,\n",
    "                      ev=True)\n",
    "checkpoint = torch.load(model_path)\n",
    "state_dict = checkpoint[\"state_dict\"]\n",
    "model = TrainingModule(model=model,\n",
    "                       lr=lr,\n",
    "                       type=type)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Extract learned cluster model weights\n",
    "cm_weights = model.model.cm.parameters()\n",
    "\n",
    "# Extract learned decoder model weights\n",
    "dm_weights = model.model.dm.parameters()\n",
    "\n",
    "# Calculate learned cluster labels\n",
    "\"\"\"\n",
    "Output learned clusters will be of shape (num_samples, num_modes)\n",
    "Ex: [0.0000, 0.0017, 0.9983] for num_modes=3 means the model learned that using 99.83% of mode 3 resulted in best performance\n",
    "\"\"\"\n",
    "cluster_probs = []\n",
    "for sample in train:\n",
    "    x = sample[0].unsqueeze(0)\n",
    "    curr_probs = model.forward(x)\n",
    "    cluster_probs.append(curr_probs.squeeze())\n",
    "cluster_probs = torch.stack(cluster_probs).detach()\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Learned cluster probabilities:\\n%s\" % cluster_probs)\n",
    "\n",
    "# Calculate final predictions\n",
    "\"\"\"\n",
    "Output predictions will be of shape (num_samples, num_muscles)\n",
    "\n",
    "\"\"\"\n",
    "model.model.ev = False\n",
    "final_preds = []\n",
    "for sample in train:\n",
    "    x = sample[0].unsqueeze(0)\n",
    "    pred = model.forward(x)\n",
    "    final_preds.append(pred.squeeze())\n",
    "final_preds = torch.stack(final_preds).detach()\n",
    "print(\"\\nFinal predictions:\\n%s\" % final_preds)\n",
    "\n",
    "# Compare final predictions to output EMG data\n",
    "train_emgs = torch.stack([sample[1] for sample in train])\n",
    "print(\"\\nEMG values:\\n%s\" % train_emgs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decoding_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
